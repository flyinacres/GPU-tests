{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ac597d2-af2d-44d5-ae7d-0cd7f2e1b23c",
   "metadata": {},
   "source": [
    "**Attempt at training on personal CUDA GPU**\n",
    "\n",
    "My local Win 11 box with a GTX 1080Ti had no problem running inference with the 2b-it version of Gemma.  \n",
    "Next test is to see if it is able to train the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9009fb4-c842-40bc-89a5-08c045f532a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'gemma_pytorch' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "# Setup the environment\n",
    "#!pip install -q -U immutabledict sentencepiece \n",
    "#!git clone https://github.com/google/gemma_pytorch.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f54fc4-a0e0-44b2-8b0f-f8fa71257e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls gemma_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5857c2d-7a0f-4763-9aff-511b0619db98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"gemma_pytorch\") \n",
    "from gemma.config import GemmaConfig, get_config_for_7b, get_config_for_2b\n",
    "from gemma.model import GemmaForCausalLM\n",
    "from gemma.tokenizer import Tokenizer\n",
    "import contextlib\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fed7249-5f31-4df9-ab7b-dc1a313bedd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ensure that this notebook is cuda-aware\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d74dec89-4ed7-4b08-bcd5-c196e5bfd851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.set_device(0)\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9df1cac0-06fb-40ca-bdfc-950d1b4639cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1080 Ti'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adf32e7-076d-4e83-98b9-5b7b64ca28d4",
   "metadata": {},
   "source": [
    "Fetch some training data from here:\n",
    "!wget -O databricks-dolly-15k.jsonl https://huggingface.co/datasets/databricks/databricks-dolly-15k/resolve/main/databricks-dolly-15k.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ca82fd5-3a5a-43f4-bb51-754ab15896d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "VARIANT = \"2b\" \n",
    "# Need to set this to cuda, not gpu or cpu while using the gpu t4 on kaggle.\n",
    "# Much faster results (as expected) when I did so.\n",
    "MACHINE_TYPE = \"cuda\" \n",
    "weights_dir = 'gemma_pytorch\\\\tokenizer' \n",
    "\n",
    "@contextlib.contextmanager\n",
    "def _set_default_tensor_type(dtype: torch.dtype):\n",
    "  \"\"\"Sets the default torch dtype to the given dtype.\"\"\"\n",
    "  torch.set_default_dtype(dtype)\n",
    "  yield\n",
    "  torch.set_default_dtype(torch.float)\n",
    "\n",
    "model_config = get_config_for_2b() if \"2b\" in VARIANT else get_config_for_7b()\n",
    "model_config.tokenizer = os.path.join(weights_dir, \"tokenizer.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67d86cd2-8f5c-4960-b7aa-1f16b498ac62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GemmaConfig(vocab_size=256000, max_position_embeddings=8192, num_hidden_layers=18, num_attention_heads=8, num_key_value_heads=1, hidden_size=2048, intermediate_size=16384, head_dim=256, rms_norm_eps=1e-06, dtype='bfloat16', quant=False, tokenizer='gemma_pytorch\\\\tokenizer\\\\tokenizer.model')\n"
     ]
    }
   ],
   "source": [
    "print(model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c38099e-17ea-4b6c-9051-629cb2d7d4f9",
   "metadata": {},
   "source": [
    "The checkpoint files (pretrained weights for 2b are available here:\n",
    "https://www.kaggle.com/models/google/gemma/frameworks/pyTorch/variations/2b?select=gemma-2b.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce36603d-2b8e-420a-80d0-9d497cdb5419",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Dev\\anaconda\\envs\\cuda_test\\Lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(MACHINE_TYPE)\n",
    "with _set_default_tensor_type(model_config.get_dtype()):\n",
    "  model = GemmaForCausalLM(model_config)\n",
    "  ckpt_path = os.path.join(weights_dir, f'gemma-{VARIANT}.ckpt')\n",
    "  model.load_weights(ckpt_path)\n",
    "  model = model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4ca7bb0-70ac-4d4b-b65b-a3d03176f803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We have different personality traits:\\n\\n* Short attention span\\n\\n* People-focused\\n\\n* Love for things to be orderly\\n\\n* Always looking at things from a broader perspective\\n* Analytical\\n\\n* Emotion-based\\n\\n* Empathizing\\n\\n* Very active (always on the move)\\n\\n* Always looking for new things\\n* Highly imaginative\\n\\n* Easily bored\\n* Focused\\n\\n* Perfectionist\\n* Task-oriented\\n\\n* Always looking for knowledge\\n* Loving\\n\\n* Patient\\n\\n* Trusting\\n\\n* Dependable\\n\\n* Logical\\n\\n* Analytical\\n\\n* Imaginative\\n\\n* Fun\\n* Warm'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the model\n",
    "\n",
    "USER_CHAT_TEMPLATE = \"<start_of_turn>user\\n{prompt}<end_of_turn>\\n\"\n",
    "MODEL_CHAT_TEMPLATE = \"<start_of_turn>model\\n{prompt}<end_of_turn>\\n\"\n",
    "\n",
    "prompt = (\n",
    "    USER_CHAT_TEMPLATE.format(\n",
    "        prompt=\"Who was president in 1852?\"\n",
    "    )\n",
    "    + \"<start_of_turn>model\\n\"\n",
    ")\n",
    "\n",
    "model.generate(\n",
    "    prompt,\n",
    "    device=device,\n",
    "    output_len=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7b269f-6cdc-4b0b-8d77-9304d29e196e",
   "metadata": {},
   "source": [
    "Note the gibberish the untrained model gives..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cuda)",
   "language": "python",
   "name": "cuda_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
