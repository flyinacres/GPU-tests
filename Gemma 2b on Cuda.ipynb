{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ac597d2-af2d-44d5-ae7d-0cd7f2e1b23c",
   "metadata": {},
   "source": [
    "**Attempt at training on personal CUDA GPU**\n",
    "\n",
    "My local Win 11 box with a GTX 1080Ti had no problem running inference with the 2b-it version of Gemma.  \n",
    "Next test is to see if it is able to train the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9009fb4-c842-40bc-89a5-08c045f532a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'gemma_pytorch'...\n",
      "The syntax of the command is incorrect.\n",
      "mv: cannot stat '/kaggle/working/gemma_pytorch/gemma/*': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Setup the environment\n",
    "!pip install -q -U immutabledict sentencepiece \n",
    "!git clone https://github.com/google/gemma_pytorch.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00f54fc4-a0e0-44b2-8b0f-f8fa71257e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTRIBUTING.md\n",
      "LICENSE\n",
      "README.md\n",
      "docker\n",
      "gemma\n",
      "requirements.txt\n",
      "scripts\n",
      "setup.py\n",
      "tokenizer\n"
     ]
    }
   ],
   "source": [
    "!ls gemma_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5857c2d-7a0f-4763-9aff-511b0619db98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"gemma_pytorch\") \n",
    "from gemma.config import GemmaConfig, get_config_for_7b, get_config_for_2b\n",
    "from gemma.model import GemmaForCausalLM\n",
    "from gemma.tokenizer import Tokenizer\n",
    "import contextlib\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fed7249-5f31-4df9-ab7b-dc1a313bedd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ensure that this notebook is cuda-aware\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d74dec89-4ed7-4b08-bcd5-c196e5bfd851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.set_device(0)\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9df1cac0-06fb-40ca-bdfc-950d1b4639cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1080 Ti'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ca82fd5-3a5a-43f4-bb51-754ab15896d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "VARIANT = \"2b-it\" \n",
    "# Need to set this to cuda, not gpu or cpu while using the gpu t4 on kaggle.\n",
    "# Much faster results (as expected) when I did so.\n",
    "MACHINE_TYPE = \"cuda\" \n",
    "weights_dir = 'gemma_pytorch\\\\tokenizer' \n",
    "\n",
    "@contextlib.contextmanager\n",
    "def _set_default_tensor_type(dtype: torch.dtype):\n",
    "  \"\"\"Sets the default torch dtype to the given dtype.\"\"\"\n",
    "  torch.set_default_dtype(dtype)\n",
    "  yield\n",
    "  torch.set_default_dtype(torch.float)\n",
    "\n",
    "model_config = get_config_for_2b() if \"2b\" in VARIANT else get_config_for_7b()\n",
    "model_config.tokenizer = os.path.join(weights_dir, \"tokenizer.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "67d86cd2-8f5c-4960-b7aa-1f16b498ac62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GemmaConfig(vocab_size=256000, max_position_embeddings=8192, num_hidden_layers=18, num_attention_heads=8, num_key_value_heads=1, hidden_size=2048, intermediate_size=16384, head_dim=256, rms_norm_eps=1e-06, dtype='bfloat16', quant=False, tokenizer='gemma_pytorch\\\\tokenizer\\\\tokenizer.model')\n"
     ]
    }
   ],
   "source": [
    "print(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ce36603d-2b8e-420a-80d0-9d497cdb5419",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Dev\\anaconda\\envs\\cuda_test\\Lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(MACHINE_TYPE)\n",
    "with _set_default_tensor_type(model_config.get_dtype()):\n",
    "  model = GemmaForCausalLM(model_config)\n",
    "  ckpt_path = os.path.join(weights_dir, f'gemma-{VARIANT}.ckpt')\n",
    "  model.load_weights(ckpt_path)\n",
    "  model = model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a4ca7bb0-70ac-4d4b-b65b-a3d03176f803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am unable to provide a specific answer without more context. The year 1852 would have been during the American Civil War era, and therefore, there would not have been a president at that time.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the model\n",
    "\n",
    "USER_CHAT_TEMPLATE = \"<start_of_turn>user\\n{prompt}<end_of_turn>\\n\"\n",
    "MODEL_CHAT_TEMPLATE = \"<start_of_turn>model\\n{prompt}<end_of_turn>\\n\"\n",
    "\n",
    "prompt = (\n",
    "    USER_CHAT_TEMPLATE.format(\n",
    "        prompt=\"Who was president in 1852?\"\n",
    "    )\n",
    "    + \"<start_of_turn>model\\n\"\n",
    ")\n",
    "\n",
    "model.generate(\n",
    "    prompt,\n",
    "    device=device,\n",
    "    output_len=300,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cuda)",
   "language": "python",
   "name": "cuda_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
