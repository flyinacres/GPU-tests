{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9009fb4-c842-40bc-89a5-08c045f532a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'gemma_pytorch'...\n",
      "The syntax of the command is incorrect.\n",
      "mv: cannot stat '/kaggle/working/gemma_pytorch/gemma/*': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Setup the environment\n",
    "!pip install -q -U immutabledict sentencepiece \n",
    "!git clone https://github.com/google/gemma_pytorch.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00f54fc4-a0e0-44b2-8b0f-f8fa71257e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTRIBUTING.md\n",
      "LICENSE\n",
      "README.md\n",
      "docker\n",
      "gemma\n",
      "requirements.txt\n",
      "scripts\n",
      "setup.py\n",
      "tokenizer\n"
     ]
    }
   ],
   "source": [
    "!ls gemma_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5857c2d-7a0f-4763-9aff-511b0619db98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"gemma_pytorch\") \n",
    "from gemma.config import GemmaConfig, get_config_for_7b, get_config_for_2b\n",
    "from gemma.model import GemmaForCausalLM\n",
    "from gemma.tokenizer import Tokenizer\n",
    "import contextlib\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fed7249-5f31-4df9-ab7b-dc1a313bedd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca82fd5-3a5a-43f4-bb51-754ab15896d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "VARIANT = \"2b-it\" \n",
    "# Need to set this to cuda, not gpu or cpu while using the gpu t4 on kaggle.\n",
    "# Much faster results (as expected) when I did so.\n",
    "MACHINE_TYPE = \"cuda\" \n",
    "weights_dir = '/kaggle/input/gemma/pytorch/2b-it/2' \n",
    "\n",
    "@contextlib.contextmanager\n",
    "def _set_default_tensor_type(dtype: torch.dtype):\n",
    "  \"\"\"Sets the default torch dtype to the given dtype.\"\"\"\n",
    "  torch.set_default_dtype(dtype)\n",
    "  yield\n",
    "  torch.set_default_dtype(torch.float)\n",
    "\n",
    "model_config = get_config_for_2b() if \"2b\" in VARIANT else get_config_for_7b()\n",
    "model_config.tokenizer = os.path.join(weights_dir, \"tokenizer.model\")\n",
    "\n",
    "device = torch.device(MACHINE_TYPE)\n",
    "with _set_default_tensor_type(model_config.get_dtype()):\n",
    "  model = GemmaForCausalLM(model_config)\n",
    "  ckpt_path = os.path.join(weights_dir, f'gemma-{VARIANT}.ckpt')\n",
    "  model.load_weights(ckpt_path)\n",
    "  model = model.to(device).eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cuda)",
   "language": "python",
   "name": "cuda_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
